{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000.0\n",
      "<class 'numpy.ndarray'>\n",
      "Their indices are  1364\n",
      "Their indices are  <class 'numpy.ndarray'>\n",
      "1364\n",
      "zero\n",
      "Their indices are------  0\n",
      "98636.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['/home/saklain/rad/balc.txt','/home/saklain/rad/door.txt','/home/saklain/rad/midroom.txt']\n",
    "with open('/home/saklain/rad/traindata.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.839827 18.593677  4.169149 ... 15.006199 22.493886 13.255091]\n",
      " [ 9.494929 20.803282  4.310922 ... 14.930078 23.816222 13.355334]\n",
      " [10.365546 21.642272  4.634742 ... 15.748691 24.289007 13.776883]\n",
      " ...\n",
      " [20.554249 18.57388  11.797091 ... 23.251001 20.868045 17.95352 ]\n",
      " [17.060326 15.42661   7.872376 ... 21.590985 20.233291 17.240152]\n",
      " [19.688239 17.789868  9.669624 ... 22.972672 21.340896 18.129271]]\n",
      "3600000\n",
      "<class 'numpy.ndarray'>\n",
      "Their indices are  (array([    80,     80,     80, ..., 221081, 221081, 221081]), array([ 1,  2,  4, ...,  8, 10, 11]))\n"
     ]
    }
   ],
   "source": [
    "#TRAINING DATA\n",
    "import numpy as np\n",
    "file = open(\"/home/saklain/rad/traindata.txt\", \"r\") \n",
    "t=0\n",
    "mainlist=[]\n",
    "mylist = []\n",
    "k=-156.535598\n",
    "cnt=0\n",
    "for line in file:\n",
    "    if(cnt==0):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    if(cnt==1):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    if(cnt==3):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    \n",
    "    t=t+1\n",
    "    cnt=cnt+1\n",
    "    if cnt==7:\n",
    "        cnt=0\n",
    "    if(t%28==0):\n",
    "        t=0\n",
    "        a=np.asarray(mylist)\n",
    "        mainlist.append(a)\n",
    "        #print(a)\n",
    "        mylist.clear()\n",
    "        #print(\"*******\")\n",
    "        \n",
    "data=np.asarray(mainlist)\n",
    "print(data)\n",
    "print(data.size)\n",
    "print(type(data))\n",
    "#data=np.delete(data, 3048, 0)\n",
    "#data=np.delete(data, 3568, 0)\n",
    "print(\"Their indices are \", np.nonzero(data == -156.535598))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000.0\n",
      "<class 'numpy.ndarray'>\n",
      "Their indices are  2051\n"
     ]
    }
   ],
   "source": [
    "print(data.size/12)\n",
    "print(type(data))\n",
    "#data=np.delete(data, 3048, 0)\n",
    "#data=np.delete(data, 3568, 0)\n",
    "print(\"test1 err \", np.unique(np.nonzero(data == -156.535598)[0]).size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000.0\n",
      "100000.0\n",
      "100000.0\n",
      "100000.0\n",
      "15000.0\n",
      "85000.0\n",
      "15000.0\n",
      "85000.0\n",
      "15000.0\n",
      "85000.0\n"
     ]
    }
   ],
   "source": [
    "print(data.size/12)\n",
    "sp = np.split(data, 3)\n",
    "data1= sp[0]\n",
    "data2= sp[1]\n",
    "data3= sp[2]\n",
    "\n",
    "print(data1.size/12)\n",
    "#print(data1)\n",
    "print(data2.size/12)\n",
    "#print(data2)\n",
    "print(data3.size/12)\n",
    "\n",
    "test1 = data1[0:15000]\n",
    "print(test1.size/12)\n",
    "train1 = data1[15000:100000]\n",
    "print(train1.size/12)\n",
    "\n",
    "\n",
    "test2 = data2[0:15000]\n",
    "print(test2.size/12)\n",
    "train2 = data2[15000:100000]\n",
    "print(train2.size/12)\n",
    "\n",
    "test3 = data3[0:15000]\n",
    "print(test3.size/12)\n",
    "train3 = data3[15000:100000]\n",
    "print(train3.size/12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1 err  367\n",
      "train1 err  997\n",
      "test2 err  192\n",
      "train2 err  297\n",
      "test3 err  197\n",
      "train3 err  1\n",
      "test1 err  14633\n",
      "train1 err  84003\n",
      "test2 err  14808\n",
      "train2 err  84703\n",
      "test3 err  14803\n",
      "train3 err  84999\n"
     ]
    }
   ],
   "source": [
    "print(\"test1 err \", np.unique(np.nonzero(test1 == -156.535598)[0]).size)\n",
    "print(\"train1 err \", np.unique(np.nonzero(train1 == -156.535598)[0]).size)\n",
    "print(\"test2 err \", np.unique(np.nonzero(test2 == -156.535598)[0]).size)\n",
    "print(\"train2 err \", np.unique(np.nonzero(train2 == -156.535598)[0]).size)\n",
    "print(\"test3 err \", np.unique(np.nonzero(test3 == -156.535598)[0]).size)\n",
    "print(\"train3 err \", np.unique(np.nonzero(train3 == -156.535598)[0]).size)\n",
    "\n",
    "print(\"test1 err \", 15000-np.unique(np.nonzero(test1 == -156.535598)[0]).size)\n",
    "print(\"train1 err \", 85000-np.unique(np.nonzero(train1 == -156.535598)[0]).size)\n",
    "print(\"test2 err \", 15000-np.unique(np.nonzero(test2 == -156.535598)[0]).size)\n",
    "print(\"train2 err \", 85000-np.unique(np.nonzero(train2 == -156.535598)[0]).size)\n",
    "print(\"test3 err \", 15000-np.unique(np.nonzero(test3 == -156.535598)[0]).size)\n",
    "print(\"train3 err \", 85000-np.unique(np.nonzero(train3 == -156.535598)[0]).size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[2 3 4]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "14633.0\n",
      "84003.0\n",
      "14808.0\n",
      "84703.0\n",
      "14803.0\n",
      "84999.0\n"
     ]
    }
   ],
   "source": [
    "while np.nonzero(train1 == -156.535598):\n",
    "    arr = np.nonzero(train1 == -156.535598)\n",
    "    #print(np.unique(arr[0]).size)\n",
    "    if np.unique(arr[0]).size==0:\n",
    "        print(\"zero\")\n",
    "        break\n",
    "    train1=np.delete(train1, np.unique(arr[0])[0], 0)\n",
    "    \n",
    "\n",
    "    \n",
    "while np.nonzero(train2 == -156.535598):\n",
    "    arr = np.nonzero(train2 == -156.535598)\n",
    "    #print(np.unique(arr[0]).size)\n",
    "    if np.unique(arr[0]).size==0:\n",
    "        print(\"zero\")\n",
    "        break\n",
    "    train2=np.delete(train2, np.unique(arr[0])[0], 0)\n",
    "    \n",
    "    \n",
    "while np.nonzero(train3 == -156.535598):\n",
    "    arr = np.nonzero(train3 == -156.535598)\n",
    "    #print(np.unique(arr[0]).size)\n",
    "    if np.unique(arr[0]).size==0:\n",
    "        print(\"zero\")\n",
    "        break\n",
    "    train3=np.delete(train3, np.unique(arr[0])[0], 0)\n",
    "    \n",
    "while np.nonzero(test1 == -156.535598):\n",
    "    arr = np.nonzero(test1 == -156.535598)\n",
    "    #print(np.unique(arr[0]).size)\n",
    "    if np.unique(arr[0]).size==0:\n",
    "        print(\"zero\")\n",
    "        break\n",
    "    test1=np.delete(test1, np.unique(arr[0])[0], 0)\n",
    "\n",
    "while np.nonzero(test2 == -156.535598):\n",
    "    arr = np.nonzero(test2 == -156.535598)\n",
    "    #print(np.unique(arr[0]).size)\n",
    "    if np.unique(arr[0]).size==0:\n",
    "        print(\"zero\")\n",
    "        break\n",
    "    test2=np.delete(test2, np.unique(arr[0])[0], 0)\n",
    "\n",
    "while np.nonzero(test3 == -156.535598):\n",
    "    arr = np.nonzero(test3 == -156.535598)\n",
    "    #print(np.unique(arr[0]).size)\n",
    "    if np.unique(arr[0]).size==0:\n",
    "        print(\"zero\")\n",
    "        break\n",
    "    test3=np.delete(test3, np.unique(arr[0])[0], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14633.0\n",
      "84003.0\n",
      "14808.0\n",
      "84703.0\n",
      "14803.0\n",
      "84999.0\n",
      "253705.0\n",
      "44244.0\n"
     ]
    }
   ],
   "source": [
    "print(test1.size/12)\n",
    "print(train1.size/12)\n",
    "\n",
    "\n",
    "print(test2.size/12)\n",
    "print(train2.size/12)\n",
    "\n",
    "print(test3.size/12)\n",
    "print(train3.size/12)\n",
    "\n",
    "trainset = np.append(train1,train2,axis=0)\n",
    "trainset = np.append(trainset,train3,axis=0)\n",
    "testset = np.append(test1,test2,axis=0)\n",
    "testset = np.append(testset,test3,axis=0)\n",
    "\n",
    "print(trainset.size/12)\n",
    "print(testset.size/12)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "253705\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "temp = []\n",
    "out = []\n",
    "\n",
    "for i in range(84003):\n",
    "    temp.append(0)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(84703):\n",
    "    temp.append(1)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(84999):\n",
    "    temp.append(2)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "        \n",
    "labels=np.asarray(out)\n",
    "print(labels)\n",
    "print(labels.size)\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "44244\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "temp = []\n",
    "out = []\n",
    "\n",
    "for i in range(14633):\n",
    "    temp.append(0)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(14808):\n",
    "    temp.append(1)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(14803):\n",
    "    temp.append(2)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "\n",
    "\n",
    "labels_test=np.asarray(out)\n",
    "print(labels_test)\n",
    "print(labels_test.size)\n",
    "print(type(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253705.0\n",
      "253705\n",
      "44244.0\n",
      "44244\n",
      "Train on 253705 samples, validate on 44244 samples\n",
      "Epoch 1/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.5454 - acc: 0.7801 - val_loss: 1.2844 - val_acc: 0.5194\n",
      "Epoch 2/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.2984 - acc: 0.9071 - val_loss: 1.5861 - val_acc: 0.4370\n",
      "Epoch 3/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.2670 - acc: 0.9133 - val_loss: 1.0892 - val_acc: 0.5859\n",
      "Epoch 4/300\n",
      "253705/253705 [==============================] - 6s 25us/step - loss: 0.2445 - acc: 0.9182 - val_loss: 1.1235 - val_acc: 0.5932\n",
      "Epoch 5/300\n",
      "253705/253705 [==============================] - 7s 26us/step - loss: 0.2313 - acc: 0.9218 - val_loss: 1.4663 - val_acc: 0.4661\n",
      "Epoch 6/300\n",
      "253705/253705 [==============================] - 5s 22us/step - loss: 0.2209 - acc: 0.9247 - val_loss: 1.2374 - val_acc: 0.5136\n",
      "Epoch 7/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.2135 - acc: 0.9270 - val_loss: 1.1288 - val_acc: 0.5620\n",
      "Epoch 8/300\n",
      "253705/253705 [==============================] - 5s 22us/step - loss: 0.2050 - acc: 0.9293 - val_loss: 1.1224 - val_acc: 0.5705\n",
      "Epoch 9/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1987 - acc: 0.9313 - val_loss: 1.2205 - val_acc: 0.5235\n",
      "Epoch 10/300\n",
      "253705/253705 [==============================] - 7s 29us/step - loss: 0.1942 - acc: 0.9340 - val_loss: 1.0849 - val_acc: 0.5812\n",
      "Epoch 11/300\n",
      "253705/253705 [==============================] - 9s 36us/step - loss: 0.1916 - acc: 0.9346 - val_loss: 1.4106 - val_acc: 0.5489\n",
      "Epoch 12/300\n",
      "253705/253705 [==============================] - 5s 20us/step - loss: 0.1893 - acc: 0.9351 - val_loss: 0.9668 - val_acc: 0.6312\n",
      "Epoch 13/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.1874 - acc: 0.9359 - val_loss: 1.1110 - val_acc: 0.5753\n",
      "Epoch 14/300\n",
      "253705/253705 [==============================] - 8s 31us/step - loss: 0.1850 - acc: 0.9373 - val_loss: 1.3762 - val_acc: 0.5386\n",
      "Epoch 15/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1828 - acc: 0.9380 - val_loss: 0.9995 - val_acc: 0.6339\n",
      "Epoch 16/300\n",
      "253705/253705 [==============================] - 8s 33us/step - loss: 0.1813 - acc: 0.9387 - val_loss: 0.8862 - val_acc: 0.6420\n",
      "Epoch 17/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1805 - acc: 0.9385 - val_loss: 0.9131 - val_acc: 0.6489\n",
      "Epoch 18/300\n",
      "253705/253705 [==============================] - 8s 30us/step - loss: 0.1790 - acc: 0.9394 - val_loss: 1.1793 - val_acc: 0.6060\n",
      "Epoch 19/300\n",
      "253705/253705 [==============================] - 8s 31us/step - loss: 0.1782 - acc: 0.9391 - val_loss: 1.1432 - val_acc: 0.5963\n",
      "Epoch 20/300\n",
      "253705/253705 [==============================] - 6s 25us/step - loss: 0.1762 - acc: 0.9401 - val_loss: 1.0001 - val_acc: 0.6421\n",
      "Epoch 21/300\n",
      "253705/253705 [==============================] - 8s 31us/step - loss: 0.1754 - acc: 0.9407 - val_loss: 0.9458 - val_acc: 0.6407\n",
      "Epoch 22/300\n",
      "253705/253705 [==============================] - 7s 28us/step - loss: 0.1756 - acc: 0.9406 - val_loss: 0.9849 - val_acc: 0.5985\n",
      "Epoch 23/300\n",
      "253705/253705 [==============================] - 7s 26us/step - loss: 0.1742 - acc: 0.9410 - val_loss: 1.1754 - val_acc: 0.5637\n",
      "Epoch 24/300\n",
      "253705/253705 [==============================] - 9s 36us/step - loss: 0.1736 - acc: 0.9413 - val_loss: 1.0308 - val_acc: 0.5899\n",
      "Epoch 25/300\n",
      "253705/253705 [==============================] - 7s 26us/step - loss: 0.1732 - acc: 0.9409 - val_loss: 0.9059 - val_acc: 0.6769\n",
      "Epoch 26/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1719 - acc: 0.9415 - val_loss: 1.0124 - val_acc: 0.5862\n",
      "Epoch 27/300\n",
      "253705/253705 [==============================] - 5s 19us/step - loss: 0.1718 - acc: 0.9418 - val_loss: 0.9208 - val_acc: 0.6432\n",
      "Epoch 28/300\n",
      "253705/253705 [==============================] - 5s 19us/step - loss: 0.1706 - acc: 0.9423 - val_loss: 1.0102 - val_acc: 0.6442\n",
      "Epoch 29/300\n",
      "253705/253705 [==============================] - 5s 20us/step - loss: 0.1714 - acc: 0.9421 - val_loss: 1.6394 - val_acc: 0.4806\n",
      "Epoch 30/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1708 - acc: 0.9424 - val_loss: 1.1597 - val_acc: 0.5760\n",
      "Epoch 31/300\n",
      "253705/253705 [==============================] - 7s 28us/step - loss: 0.1707 - acc: 0.9428 - val_loss: 1.0214 - val_acc: 0.5806\n",
      "Epoch 32/300\n",
      "253705/253705 [==============================] - 8s 33us/step - loss: 0.1708 - acc: 0.9422 - val_loss: 0.9688 - val_acc: 0.6898\n",
      "Epoch 33/300\n",
      "253705/253705 [==============================] - 8s 31us/step - loss: 0.1698 - acc: 0.9426 - val_loss: 0.9609 - val_acc: 0.6351\n",
      "Epoch 34/300\n",
      "253705/253705 [==============================] - 8s 32us/step - loss: 0.1701 - acc: 0.9426 - val_loss: 1.2214 - val_acc: 0.5425\n",
      "Epoch 35/300\n",
      "253705/253705 [==============================] - 7s 28us/step - loss: 0.1690 - acc: 0.9431 - val_loss: 0.8832 - val_acc: 0.6423\n",
      "Epoch 36/300\n",
      "253705/253705 [==============================] - 8s 33us/step - loss: 0.1686 - acc: 0.9431 - val_loss: 1.5489 - val_acc: 0.5363\n",
      "Epoch 37/300\n",
      "253705/253705 [==============================] - 8s 32us/step - loss: 0.1687 - acc: 0.9434 - val_loss: 1.2124 - val_acc: 0.5512\n",
      "Epoch 38/300\n",
      "253705/253705 [==============================] - 6s 25us/step - loss: 0.1686 - acc: 0.9429 - val_loss: 1.1159 - val_acc: 0.5495\n",
      "Epoch 39/300\n",
      "253705/253705 [==============================] - 7s 29us/step - loss: 0.1676 - acc: 0.9434 - val_loss: 1.2638 - val_acc: 0.5583\n",
      "Epoch 40/300\n",
      "253705/253705 [==============================] - 8s 33us/step - loss: 0.1681 - acc: 0.9434 - val_loss: 1.0682 - val_acc: 0.6119\n",
      "Epoch 41/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1678 - acc: 0.9433 - val_loss: 1.1637 - val_acc: 0.5647\n",
      "Epoch 42/300\n",
      "253705/253705 [==============================] - 7s 27us/step - loss: 0.1669 - acc: 0.9441 - val_loss: 1.0356 - val_acc: 0.5644\n",
      "Epoch 43/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1664 - acc: 0.9439 - val_loss: 0.9840 - val_acc: 0.5952\n",
      "Epoch 44/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1666 - acc: 0.9441 - val_loss: 0.9862 - val_acc: 0.6134\n",
      "Epoch 45/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1660 - acc: 0.9447 - val_loss: 1.0780 - val_acc: 0.5740\n",
      "Epoch 46/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1661 - acc: 0.9443 - val_loss: 1.0531 - val_acc: 0.6185\n",
      "Epoch 47/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1664 - acc: 0.9449 - val_loss: 1.4871 - val_acc: 0.5356\n",
      "Epoch 48/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1658 - acc: 0.9444 - val_loss: 1.0166 - val_acc: 0.5918\n",
      "Epoch 49/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1649 - acc: 0.9453 - val_loss: 1.0087 - val_acc: 0.6064\n",
      "Epoch 50/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1647 - acc: 0.9454 - val_loss: 0.9629 - val_acc: 0.6501\n",
      "Epoch 51/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1640 - acc: 0.9455 - val_loss: 1.2464 - val_acc: 0.5589\n",
      "Epoch 52/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1631 - acc: 0.9459 - val_loss: 1.2666 - val_acc: 0.5898\n",
      "Epoch 53/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.1626 - acc: 0.9458 - val_loss: 1.1232 - val_acc: 0.5968\n",
      "Epoch 54/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1623 - acc: 0.9461 - val_loss: 1.1369 - val_acc: 0.5976\n",
      "Epoch 55/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1613 - acc: 0.9464 - val_loss: 1.1918 - val_acc: 0.6042\n",
      "Epoch 56/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1615 - acc: 0.9468 - val_loss: 1.4387 - val_acc: 0.5384\n",
      "Epoch 57/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1605 - acc: 0.9466 - val_loss: 1.1157 - val_acc: 0.5778\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1601 - acc: 0.9470 - val_loss: 1.0704 - val_acc: 0.6111\n",
      "Epoch 59/300\n",
      "253705/253705 [==============================] - 6s 22us/step - loss: 0.1595 - acc: 0.9473 - val_loss: 1.2932 - val_acc: 0.5856\n",
      "Epoch 60/300\n",
      "253705/253705 [==============================] - 6s 24us/step - loss: 0.1592 - acc: 0.9480 - val_loss: 1.1311 - val_acc: 0.5958\n",
      "Epoch 61/300\n",
      "253705/253705 [==============================] - 6s 23us/step - loss: 0.1590 - acc: 0.9478 - val_loss: 1.1530 - val_acc: 0.6017\n",
      "Epoch 62/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.1588 - acc: 0.9482 - val_loss: 1.0285 - val_acc: 0.6404\n",
      "Epoch 63/300\n",
      "253705/253705 [==============================] - 5s 19us/step - loss: 0.1579 - acc: 0.9490 - val_loss: 1.3248 - val_acc: 0.6072\n",
      "Epoch 64/300\n",
      "253705/253705 [==============================] - 5s 20us/step - loss: 0.1569 - acc: 0.9486 - val_loss: 1.0791 - val_acc: 0.6193\n",
      "Epoch 65/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.1567 - acc: 0.9496 - val_loss: 1.0762 - val_acc: 0.6104\n",
      "Epoch 66/300\n",
      "253705/253705 [==============================] - 5s 20us/step - loss: 0.1566 - acc: 0.9494 - val_loss: 1.0693 - val_acc: 0.5844\n",
      "Epoch 67/300\n",
      "253705/253705 [==============================] - 5s 21us/step - loss: 0.1560 - acc: 0.9494 - val_loss: 1.1449 - val_acc: 0.6184\n",
      "Epoch 68/300\n",
      "253705/253705 [==============================] - 5s 19us/step - loss: 0.1565 - acc: 0.9492 - val_loss: 1.2133 - val_acc: 0.6027\n",
      "Epoch 69/300\n",
      "253705/253705 [==============================] - 5s 19us/step - loss: 0.1554 - acc: 0.9498 - val_loss: 1.1222 - val_acc: 0.6297\n",
      "Epoch 70/300\n",
      "212768/253705 [========================>.....] - ETA: 0s - loss: 0.1561 - acc: 0.9494"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=12))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.add(Dense(32, activation='softmax'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(trainset.size/12)\n",
    "print(labels.size)\n",
    "print(testset.size/12)\n",
    "print(labels_test.size)\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=3)\n",
    "one_hot_labels_test = keras.utils.to_categorical(labels_test, num_classes=3)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(trainset, one_hot_labels, epochs=300,validation_data=(testset, one_hot_labels_test), batch_size=32)\n",
    "model.save('/home/saklain/ThesisCNN/newmodel.h5')\n",
    "\n",
    "#score = model.evaluate(data_test,one_hot_labels_test, batch_size=32)\n",
    "#print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44244/44244 [==============================] - 0s 5us/step\n",
      "[1.1294365176325971, 0.5302413886628695]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testset,one_hot_labels_test, batch_size=32)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
