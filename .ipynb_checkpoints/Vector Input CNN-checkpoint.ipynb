{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24.182945 16.145279  9.571236 ... 25.770516 20.060344 17.027496]\n",
      " [23.979383 16.511075 10.021134 ... 25.565212 20.267513 17.223805]\n",
      " [23.855664 16.417096  9.228363 ... 25.515045 20.124928 17.133437]\n",
      " ...\n",
      " [19.474705 17.384317 10.017467 ... 22.711546 22.258822 16.114474]\n",
      " [18.792176 17.840203  7.941058 ... 22.621892 22.412922 15.693076]\n",
      " [14.599934 19.159947  4.537022 ... 17.191217 22.993511 12.678775]]\n",
      "54000\n",
      "<class 'numpy.ndarray'>\n",
      "Their indices are  (array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#TRAINING DATA\n",
    "import numpy as np\n",
    "file = open(\"/home/saklain/csidata2/dataset.txt\", \"r\") \n",
    "t=0\n",
    "mainlist=[]\n",
    "mylist = []\n",
    "k=-156.535598\n",
    "cnt=0\n",
    "for line in file:\n",
    "    if(cnt==0):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    if(cnt==1):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    if(cnt==3):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    \n",
    "    t=t+1\n",
    "    cnt=cnt+1\n",
    "    if cnt==7:\n",
    "        cnt=0\n",
    "    if(t%28==0):\n",
    "        t=0\n",
    "        a=np.asarray(mylist)\n",
    "        mainlist.append(a)\n",
    "        #print(a)\n",
    "        mylist.clear()\n",
    "        #print(\"*******\")\n",
    "        \n",
    "data=np.asarray(mainlist)\n",
    "print(data)\n",
    "print(data.size)\n",
    "print(type(data))\n",
    "data=np.delete(data, 3048, 0)\n",
    "data=np.delete(data, 3568, 0)\n",
    "print(\"Their indices are \", np.nonzero(data == -156.535598))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "[[ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(arr)\n",
    "arr=np.delete(arr, 0, 0)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.921939 15.213217 11.91579  ... 24.344346 20.544173 18.121561]\n",
      " [22.28287  16.131366 12.722352 ... 24.437043 20.038219 17.685699]\n",
      " [22.276048 16.062883 11.972439 ... 24.520864 19.740388 17.561846]\n",
      " ...\n",
      " [16.461951 21.629622 11.769031 ... 20.102226 24.127926 15.614083]\n",
      " [20.898464 20.268442  7.519211 ... 23.782958 23.035892 13.927544]\n",
      " [19.135207 21.132562  9.14371  ... 22.649367 23.966153 15.679335]]\n",
      "7200\n",
      "<class 'numpy.ndarray'>\n",
      "Their indices are  (array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "file = open(\"/home/saklain/csidata2/testset.txt\", \"r\") \n",
    "t=0\n",
    "mainlist=[]\n",
    "mylist = []\n",
    "k=-156.535598\n",
    "cnt=0\n",
    "for line in file:\n",
    "    if(cnt==0):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    if(cnt==1):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    if(cnt==3):\n",
    "        line=float(line.replace(\"\\n\",\"\"))\n",
    "        mylist.append(line)\n",
    "    \n",
    "    t=t+1\n",
    "    cnt=cnt+1\n",
    "    if cnt==7:\n",
    "        cnt=0\n",
    "    if(t%28==0):\n",
    "        t=0\n",
    "        a=np.asarray(mylist)\n",
    "        mainlist.append(a)\n",
    "        #print(a)\n",
    "        mylist.clear()\n",
    "        #print(\"*******\")\n",
    "        \n",
    "data_test=np.asarray(mainlist)\n",
    "print(data_test)\n",
    "print(data_test.size)\n",
    "print(type(data_test))\n",
    "print(\"Their indices are \", np.nonzero(data_test == -156.535598))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "4498\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "temp = []\n",
    "out = []\n",
    "\n",
    "for i in range(1500):\n",
    "    temp.append(0)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(1500):\n",
    "    temp.append(1)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(1498):\n",
    "    temp.append(2)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "        \n",
    "labels=np.asarray(out)\n",
    "print(labels)\n",
    "print(labels.size)\n",
    "print(type(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "600\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "temp = []\n",
    "out = []\n",
    "\n",
    "for i in range(200):\n",
    "    temp.append(0)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(200):\n",
    "    temp.append(1)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "    \n",
    "for i in range(200):\n",
    "    temp.append(2)\n",
    "    tempfin=np.asarray(temp)\n",
    "    out.append(tempfin)\n",
    "    temp.clear()\n",
    "\n",
    "\n",
    "labels_test=np.asarray(out)\n",
    "print(labels_test)\n",
    "print(labels_test.size)\n",
    "print(type(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53976\n",
      "4498\n",
      "7200\n",
      "600\n",
      "Train on 4498 samples, validate on 600 samples\n",
      "Epoch 1/50\n",
      "4498/4498 [==============================] - 0s 61us/step - loss: 1.0230 - acc: 0.4989 - val_loss: 0.7295 - val_acc: 0.6067\n",
      "Epoch 2/50\n",
      "4498/4498 [==============================] - 0s 64us/step - loss: 0.6231 - acc: 0.6661 - val_loss: 0.7046 - val_acc: 0.6717\n",
      "Epoch 3/50\n",
      "4498/4498 [==============================] - 0s 67us/step - loss: 0.5594 - acc: 0.6994 - val_loss: 0.6325 - val_acc: 0.6200\n",
      "Epoch 4/50\n",
      "4498/4498 [==============================] - 0s 69us/step - loss: 0.5266 - acc: 0.7199 - val_loss: 0.6208 - val_acc: 0.6133\n",
      "Epoch 5/50\n",
      "4498/4498 [==============================] - 0s 59us/step - loss: 0.4998 - acc: 0.7401 - val_loss: 0.6336 - val_acc: 0.6683\n",
      "Epoch 6/50\n",
      "4498/4498 [==============================] - 0s 62us/step - loss: 0.4884 - acc: 0.7434 - val_loss: 0.5899 - val_acc: 0.6900\n",
      "Epoch 7/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.4672 - acc: 0.7663 - val_loss: 0.6708 - val_acc: 0.6483\n",
      "Epoch 8/50\n",
      "4498/4498 [==============================] - 0s 59us/step - loss: 0.4562 - acc: 0.7679 - val_loss: 0.6238 - val_acc: 0.6800\n",
      "Epoch 9/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.4475 - acc: 0.7808 - val_loss: 0.7595 - val_acc: 0.6433\n",
      "Epoch 10/50\n",
      "4498/4498 [==============================] - 0s 72us/step - loss: 0.4402 - acc: 0.7786 - val_loss: 0.6229 - val_acc: 0.6833\n",
      "Epoch 11/50\n",
      "4498/4498 [==============================] - 0s 62us/step - loss: 0.4340 - acc: 0.7770 - val_loss: 0.5778 - val_acc: 0.6783\n",
      "Epoch 12/50\n",
      "4498/4498 [==============================] - 0s 59us/step - loss: 0.4277 - acc: 0.7861 - val_loss: 0.5987 - val_acc: 0.6467\n",
      "Epoch 13/50\n",
      "4498/4498 [==============================] - 0s 67us/step - loss: 0.4163 - acc: 0.7986 - val_loss: 0.7291 - val_acc: 0.6450\n",
      "Epoch 14/50\n",
      "4498/4498 [==============================] - 0s 67us/step - loss: 0.4121 - acc: 0.8037 - val_loss: 0.7655 - val_acc: 0.6467\n",
      "Epoch 15/50\n",
      "4498/4498 [==============================] - 0s 77us/step - loss: 0.4049 - acc: 0.8037 - val_loss: 0.6490 - val_acc: 0.7067\n",
      "Epoch 16/50\n",
      "4498/4498 [==============================] - 0s 68us/step - loss: 0.4028 - acc: 0.8064 - val_loss: 0.5533 - val_acc: 0.7533\n",
      "Epoch 17/50\n",
      "4498/4498 [==============================] - 0s 67us/step - loss: 0.4001 - acc: 0.8024 - val_loss: 0.6329 - val_acc: 0.6367\n",
      "Epoch 18/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.3885 - acc: 0.8150 - val_loss: 0.6277 - val_acc: 0.6767\n",
      "Epoch 19/50\n",
      "4498/4498 [==============================] - 0s 59us/step - loss: 0.3905 - acc: 0.8064 - val_loss: 0.6972 - val_acc: 0.6350\n",
      "Epoch 20/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.3829 - acc: 0.8235 - val_loss: 0.6614 - val_acc: 0.6583\n",
      "Epoch 21/50\n",
      "4498/4498 [==============================] - 0s 63us/step - loss: 0.3864 - acc: 0.8199 - val_loss: 0.5840 - val_acc: 0.6650\n",
      "Epoch 22/50\n",
      "4498/4498 [==============================] - 0s 67us/step - loss: 0.3773 - acc: 0.8190 - val_loss: 0.5688 - val_acc: 0.7133\n",
      "Epoch 23/50\n",
      "4498/4498 [==============================] - 0s 76us/step - loss: 0.3778 - acc: 0.8199 - val_loss: 0.5699 - val_acc: 0.7000\n",
      "Epoch 24/50\n",
      "4498/4498 [==============================] - 0s 61us/step - loss: 0.3749 - acc: 0.8261 - val_loss: 0.5232 - val_acc: 0.7383\n",
      "Epoch 25/50\n",
      "4498/4498 [==============================] - 0s 80us/step - loss: 0.3702 - acc: 0.8297 - val_loss: 0.6064 - val_acc: 0.7250\n",
      "Epoch 26/50\n",
      "4498/4498 [==============================] - 0s 69us/step - loss: 0.3669 - acc: 0.8353 - val_loss: 0.7610 - val_acc: 0.6233\n",
      "Epoch 27/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.3649 - acc: 0.8264 - val_loss: 0.6066 - val_acc: 0.6683\n",
      "Epoch 28/50\n",
      "4498/4498 [==============================] - 0s 76us/step - loss: 0.3593 - acc: 0.8326 - val_loss: 0.5811 - val_acc: 0.6967\n",
      "Epoch 29/50\n",
      "4498/4498 [==============================] - 0s 70us/step - loss: 0.3573 - acc: 0.8375 - val_loss: 0.6228 - val_acc: 0.6567\n",
      "Epoch 30/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.3566 - acc: 0.8286 - val_loss: 0.5442 - val_acc: 0.7467\n",
      "Epoch 31/50\n",
      "4498/4498 [==============================] - 0s 69us/step - loss: 0.3595 - acc: 0.8266 - val_loss: 0.5961 - val_acc: 0.6800\n",
      "Epoch 32/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.3512 - acc: 0.8324 - val_loss: 0.5355 - val_acc: 0.7050\n",
      "Epoch 33/50\n",
      "4498/4498 [==============================] - 0s 75us/step - loss: 0.3519 - acc: 0.8370 - val_loss: 0.5711 - val_acc: 0.7933\n",
      "Epoch 34/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.3436 - acc: 0.8448 - val_loss: 0.5307 - val_acc: 0.7683\n",
      "Epoch 35/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.3501 - acc: 0.8382 - val_loss: 0.5257 - val_acc: 0.7867\n",
      "Epoch 36/50\n",
      "4498/4498 [==============================] - 0s 63us/step - loss: 0.3499 - acc: 0.8364 - val_loss: 0.5645 - val_acc: 0.7217\n",
      "Epoch 37/50\n",
      "4498/4498 [==============================] - 0s 69us/step - loss: 0.3457 - acc: 0.8379 - val_loss: 0.7295 - val_acc: 0.6567\n",
      "Epoch 38/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.3424 - acc: 0.8464 - val_loss: 0.5921 - val_acc: 0.6867\n",
      "Epoch 39/50\n",
      "4498/4498 [==============================] - 0s 62us/step - loss: 0.3349 - acc: 0.8444 - val_loss: 0.6368 - val_acc: 0.6617\n",
      "Epoch 40/50\n",
      "4498/4498 [==============================] - 0s 66us/step - loss: 0.3413 - acc: 0.8390 - val_loss: 0.6229 - val_acc: 0.6967\n",
      "Epoch 41/50\n",
      "4498/4498 [==============================] - 0s 63us/step - loss: 0.3318 - acc: 0.8437 - val_loss: 0.6176 - val_acc: 0.7283\n",
      "Epoch 42/50\n",
      "4498/4498 [==============================] - 0s 72us/step - loss: 0.3374 - acc: 0.8468 - val_loss: 0.5073 - val_acc: 0.7750\n",
      "Epoch 43/50\n",
      "4498/4498 [==============================] - 0s 74us/step - loss: 0.3373 - acc: 0.8446 - val_loss: 0.5941 - val_acc: 0.7200\n",
      "Epoch 44/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.3320 - acc: 0.8493 - val_loss: 0.6710 - val_acc: 0.6867\n",
      "Epoch 45/50\n",
      "4498/4498 [==============================] - 0s 58us/step - loss: 0.3336 - acc: 0.8450 - val_loss: 0.6690 - val_acc: 0.6567\n",
      "Epoch 46/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.3280 - acc: 0.8444 - val_loss: 0.6484 - val_acc: 0.6667\n",
      "Epoch 47/50\n",
      "4498/4498 [==============================] - 0s 69us/step - loss: 0.3283 - acc: 0.8495 - val_loss: 0.6424 - val_acc: 0.6783\n",
      "Epoch 48/50\n",
      "4498/4498 [==============================] - 0s 71us/step - loss: 0.3348 - acc: 0.8430 - val_loss: 0.5843 - val_acc: 0.7817\n",
      "Epoch 49/50\n",
      "4498/4498 [==============================] - 0s 65us/step - loss: 0.3341 - acc: 0.8430 - val_loss: 0.5704 - val_acc: 0.7033\n",
      "Epoch 50/50\n",
      "4498/4498 [==============================] - 0s 78us/step - loss: 0.3291 - acc: 0.8484 - val_loss: 0.5457 - val_acc: 0.7567\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=12))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(data.size)\n",
    "print(labels.size)\n",
    "print(data_test.size)\n",
    "print(labels_test.size)\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=3)\n",
    "one_hot_labels_test = keras.utils.to_categorical(labels_test, num_classes=3)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=50,validation_data=(data_test, one_hot_labels_test), batch_size=32)\n",
    "model.save('/home/saklain/ThesisCNN/mymodel.h5')\n",
    "\n",
    "#score = model.evaluate(data_test,one_hot_labels_test, batch_size=32)\n",
    "#print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 33us/step\n",
      "[0.545741320202748, 0.7566666666666667]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(data_test,one_hot_labels_test, batch_size=32)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [2]\n",
      " [5]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [4]]\n",
      "10000\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#data = np.random.random((10000, 12))\n",
    "#labels = np.random.randint(6, size=(10000, 1))\n",
    "\n",
    "\n",
    "\n",
    "data_test = np.random.random((10000, 12))\n",
    "labels_test = np.random.randint(6, size=(10000, 1))\n",
    "print(labels_test)\n",
    "print(labels_test.size)\n",
    "print(type(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53976\n",
      "4498\n",
      "120000\n",
      "10000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d30582ebf069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Convert labels to categorical one-hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mone_hot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mone_hot_labels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Train the model, iterating on the data in batches of 32 samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.initializers import glorot_uniform\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=12, kernel_initializer=glorot_uniform(seed=1)))\n",
    "model.add(Dense(3, activation='softmax', kernel_initializer=glorot_uniform(seed=1)))\n",
    "opt = keras.optimizers.RMSprop(lr=0.005)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(data.size)\n",
    "print(labels.size)\n",
    "print(data_test.size)\n",
    "print(labels_test.size)\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=3)\n",
    "one_hot_labels_test = keras.utils.to_categorical(labels_test, num_classes=3)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=30, validation_data=(data_test, one_hot_labels_test),batch_size=32)\n",
    "model.save('/home/saklain/ThesisCNN/mymodel.h5')\n",
    "\n",
    "score = model.evaluate(data_test,one_hot_labels_test, batch_size=32)\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
