{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chunk = 5\n",
    "div=1\n",
    "\n",
    "\n",
    "#READ data from file and parse the valid values \n",
    "def parseCsi(filename):\n",
    "    file = open(filename, \"r\") \n",
    "    t=0\n",
    "    mainlist=[]\n",
    "    mylist = []\n",
    "    #k=-156.535598\n",
    "    cnt=0\n",
    "    for line in file:\n",
    "        if(cnt==0):\n",
    "            line=float(line.replace(\"\\n\",\"\"))\n",
    "            mylist.append(line)\n",
    "        if(cnt==1):\n",
    "            line=float(line.replace(\"\\n\",\"\"))\n",
    "            mylist.append(line)\n",
    "        if(cnt==3):\n",
    "            line=float(line.replace(\"\\n\",\"\"))\n",
    "            mylist.append(line)\n",
    "\n",
    "        t=t+1\n",
    "        cnt=cnt+1\n",
    "        if cnt==7:\n",
    "            cnt=0\n",
    "        if(t%28==0):\n",
    "            t=0\n",
    "            a=np.asarray(mylist)\n",
    "            mainlist.append(a)\n",
    "            #print(a)\n",
    "            mylist.clear()\n",
    "            #print(\"*******\")\n",
    "\n",
    "    data_=np.asarray(mainlist)\n",
    "    print(data_.size/12)\n",
    "    print(type(data_))\n",
    "    #print(\"Their indices are \", np.nonzero(data_ == -156.535598))\n",
    "    \n",
    "    \n",
    "    while np.nonzero(data_ == -156.535598):\n",
    "        arr = np.nonzero(data_ == -156.535598)\n",
    "        #print(np.unique(arr[0]).size)\n",
    "        if np.unique(arr[0]).size==0:\n",
    "            print(\"zero\")\n",
    "            break\n",
    "        data_=np.delete(data_, np.unique(arr[0])[0], 0)\n",
    "\n",
    "    print(data_.size/12)\n",
    "\n",
    "\n",
    "    data_= data_[:int(data_.size/div)]\n",
    "    k= int(data_.size/12-(data_.size/12)%chunk)\n",
    "    print(k)\n",
    "    data_ = data_[:k]\n",
    "    length= int(data_.size/12)\n",
    "    print(data_.size/12,length)\n",
    "    \n",
    "    return data_\n",
    "    #print(\"Their indices are \", np.nonzero(data_ == -156.535598))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendData(data1,data2):    \n",
    "    import numpy as np\n",
    "    temp = []\n",
    "    out = []\n",
    "\n",
    "    data = np.append(data1,data2,axis=0)\n",
    "    print(data.size/12)\n",
    "\n",
    "    len_d1 = int(data1.size/12)\n",
    "    len_d2 = int(data2.size/12)\n",
    "    \n",
    "    print(data[len_d1])\n",
    "    print(data[len_d1+5])\n",
    "\n",
    "    data = np.asarray(np.reshape(data,(-1,chunk,12)))\n",
    "\n",
    "    print(data.size/12/chunk)\n",
    "    print(data[0])\n",
    "\n",
    "    for i in range(int(len_d1/chunk)):\n",
    "        temp.append(0)\n",
    "        tempfin=np.asarray(temp)\n",
    "        out.append(tempfin)\n",
    "        temp.clear()\n",
    "\n",
    "    for i in range(int(len_d2/chunk)):\n",
    "        temp.append(1)\n",
    "        tempfin=np.asarray(temp)\n",
    "        out.append(tempfin)\n",
    "        temp.clear()\n",
    "\n",
    "    labels=np.asarray(out)\n",
    "    print(labels)\n",
    "    print(labels.size)\n",
    "    print(type(labels))\n",
    "\n",
    "    print(labels[int(len_d1/chunk)])\n",
    "    \n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000.0\n",
      "<class 'numpy.ndarray'>\n",
      "zero\n",
      "9492.0\n",
      "9490\n",
      "9490.0 9490\n",
      "10000.0\n",
      "<class 'numpy.ndarray'>\n",
      "zero\n",
      "9986.0\n",
      "9985\n",
      "9985.0 9985\n",
      "19475.0\n",
      "[21.770826 22.525675  5.091766 21.867048 22.60681   6.461439 22.552848\n",
      " 23.184445 10.174864 24.42038  24.662104 13.112535]\n",
      "[23.035381 19.913238 11.89094  23.107666 20.059233 12.832476 23.61016\n",
      " 21.04313  15.924986 24.938469 22.757301 17.626805]\n",
      "3895.0\n",
      "[[22.351992 10.735952  6.363993 22.436378 11.590436  8.030463 23.028951\n",
      "  13.594442 11.036127 24.527889 14.173416 13.986722]\n",
      " [22.292324 11.528575  6.175014 22.377856 12.282758  8.121444 22.993133\n",
      "  14.252614 11.354333 24.595117 14.83659  14.200922]\n",
      " [21.203509 10.75481   6.421485 21.312846 11.653814  7.994036 22.07116\n",
      "  13.306491 10.888473 23.879745 13.722602 13.803936]\n",
      " [22.317795 11.333368  6.47813  22.402836 12.214238  8.233121 23.013557\n",
      "  14.499012 11.509208 24.590224 15.112401 14.297645]\n",
      " [21.571177 10.877472  6.276124 21.671824 11.766878  7.880512 22.353448\n",
      "  13.574433 10.867687 24.013081 14.104697 13.759993]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "3895\n",
      "<class 'numpy.ndarray'>\n",
      "[1]\n",
      "1000.0\n",
      "<class 'numpy.ndarray'>\n",
      "zero\n",
      "903.0\n",
      "900\n",
      "900.0 900\n",
      "1000.0\n",
      "<class 'numpy.ndarray'>\n",
      "zero\n",
      "688.0\n",
      "685\n",
      "685.0 685\n",
      "1585.0\n",
      "[16.6988   13.838146  3.922852 16.994991 14.380428  5.722843 18.681466\n",
      " 17.07624   9.718827 21.178671 20.912906 12.063229]\n",
      "[19.988576 19.197708  8.177842 20.132105 19.368911  9.600898 21.044626\n",
      " 20.499943 13.17112  22.475905 23.018689 16.100966]\n",
      "317.0\n",
      "[[20.418824 13.725961 10.992618 20.549216 14.28085  11.993981 21.445869\n",
      "  16.860018 14.923408 23.676784 18.933145 16.968651]\n",
      " [21.76019  18.283129 11.169647 21.856643 18.492676 12.25696  22.543918\n",
      "  19.759199 16.220419 24.410234 22.211139 19.370927]\n",
      " [21.673767 18.343129 11.574182 21.772117 18.549955 12.5619   22.471483\n",
      "  19.861437 16.182977 24.373932 22.360671 19.395152]\n",
      " [21.709529 18.286125 10.485171 21.807089 18.495564 11.71898  22.50143\n",
      "  19.80351  16.021903 24.385837 22.270521 19.339307]\n",
      " [21.434187 18.388178 11.228401 21.537993 18.592961 12.303556 22.271845\n",
      "  19.883145 16.23357  24.233229 22.312458 19.36328 ]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "317\n",
      "<class 'numpy.ndarray'>\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "data1 = parseCsi(\"dev1.txt\")\n",
    "data2 = parseCsi(\"dev2.txt\")\n",
    "\n",
    "dataset,labels = appendData(data1,data2)\n",
    "\n",
    "testdata1 = parseCsi(\"dev1t.txt\")\n",
    "testdata2 = parseCsi(\"dev2t.txt\")\n",
    "\n",
    "testdata,testlabels = appendData(testdata1,testdata2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saklain/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense\n",
    "import keras\n",
    "\n",
    "time_steps = chunk\n",
    "feature_length = 12\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(time_steps, feature_length)))\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "#model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=2)\n",
    "one_hot_labels_test = keras.utils.to_categorical(testlabels, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3116 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "3116/3116 [==============================] - 5s 2ms/step - loss: 0.1828 - acc: 0.9955 - val_loss: 0.2601 - val_acc: 0.9756\n",
      "Epoch 2/10\n",
      "3116/3116 [==============================] - 4s 1ms/step - loss: 0.1707 - acc: 0.9978 - val_loss: 0.2308 - val_acc: 0.9859\n",
      "Epoch 3/10\n",
      "3116/3116 [==============================] - 4s 1ms/step - loss: 0.1601 - acc: 0.9987 - val_loss: 0.2449 - val_acc: 0.9743\n",
      "Epoch 4/10\n",
      "3116/3116 [==============================] - 5s 2ms/step - loss: 0.1536 - acc: 0.9968 - val_loss: 0.2590 - val_acc: 0.9589TA:\n",
      "Epoch 5/10\n",
      "3116/3116 [==============================] - 4s 1ms/step - loss: 0.1432 - acc: 0.9987 - val_loss: 0.2470 - val_acc: 0.9615\n",
      "Epoch 6/10\n",
      "3116/3116 [==============================] - 4s 1ms/step - loss: 0.1365 - acc: 0.9981 - val_loss: 0.2000 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "3116/3116 [==============================] - 5s 2ms/step - loss: 0.1294 - acc: 0.9981 - val_loss: 0.2100 - val_acc: 0.9756\n",
      "Epoch 8/10\n",
      "3116/3116 [==============================] - 5s 2ms/step - loss: 0.1247 - acc: 0.9971 - val_loss: 0.2352 - val_acc: 0.9551\n",
      "Epoch 9/10\n",
      "3116/3116 [==============================] - 5s 2ms/step - loss: 0.1168 - acc: 0.9978 - val_loss: 0.2574 - val_acc: 0.9448\n",
      "Epoch 10/10\n",
      "3116/3116 [==============================] - 5s 2ms/step - loss: 0.1107 - acc: 0.9981 - val_loss: 0.1975 - val_acc: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32fd338588>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, one_hot_labels , epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "#model.fit(dataset, one_hot_labels, epochs=10,validation_data=(testdata, one_hot_labels_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('/home/saklain/ThesisCNN/modelauth.h5')\n",
    "\n",
    "score = model.evaluate(testdata, one_hot_labels_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************SET***************::::::: 1\n",
      "Train on 3505 samples, validate on 390 samples\n",
      "Epoch 1/10\n",
      "3505/3505 [==============================] - 5s 2ms/step - loss: 0.1162 - acc: 0.9966 - val_loss: 0.1152 - val_acc: 0.9872\n",
      "Epoch 2/10\n",
      "3505/3505 [==============================] - 4s 1ms/step - loss: 0.1068 - acc: 0.9960 - val_loss: 0.1167 - val_acc: 0.9897\n",
      "Epoch 3/10\n",
      "3505/3505 [==============================] - 4s 1ms/step - loss: 0.1021 - acc: 0.9954 - val_loss: 0.1308 - val_acc: 0.9795\n",
      "Epoch 4/10\n",
      "3505/3505 [==============================] - 6s 2ms/step - loss: 0.0928 - acc: 0.9969 - val_loss: 0.1223 - val_acc: 0.9846\n",
      "Epoch 5/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0901 - acc: 0.9963 - val_loss: 0.1149 - val_acc: 0.9897\n",
      "Epoch 6/10\n",
      "3505/3505 [==============================] - 6s 2ms/step - loss: 0.0848 - acc: 0.9963 - val_loss: 0.1034 - val_acc: 0.9897\n",
      "Epoch 7/10\n",
      "3505/3505 [==============================] - 4s 1ms/step - loss: 0.0808 - acc: 0.9971 - val_loss: 0.1247 - val_acc: 0.9821\n",
      "Epoch 8/10\n",
      "3505/3505 [==============================] - 6s 2ms/step - loss: 0.0775 - acc: 0.9960 - val_loss: 0.2243 - val_acc: 0.9436\n",
      "Epoch 9/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0741 - acc: 0.9966 - val_loss: 0.1356 - val_acc: 0.9795\n",
      "Epoch 10/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0732 - acc: 0.9951 - val_loss: 0.1083 - val_acc: 0.9872\n",
      "**************SET***************::::::: 2\n",
      "Train on 3505 samples, validate on 390 samples\n",
      "Epoch 1/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0740 - acc: 0.9937 - val_loss: 0.0685 - val_acc: 1.0000 acc: 0 - ETA: 0s - loss: 0.0745 - acc:\n",
      "Epoch 2/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0727 - acc: 0.9926 - val_loss: 0.0652 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0641 - acc: 0.9957 - val_loss: 0.0629 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "3505/3505 [==============================] - 6s 2ms/step - loss: 0.0649 - acc: 0.9934 - val_loss: 0.0634 - val_acc: 0.9974\n",
      "Epoch 5/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0617 - acc: 0.9949 - val_loss: 0.0554 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "3505/3505 [==============================] - 5s 2ms/step - loss: 0.0566 - acc: 0.9963 - val_loss: 0.0773 - val_acc: 0.9897\n",
      "Epoch 7/10\n",
      "3505/3505 [==============================] - 6s 2ms/step - loss: 0.0564 - acc: 0.9946 - val_loss: 0.0526 - val_acc: 0.9974\n",
      "Epoch 8/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0539 - acc: 0.9946 - val_loss: 0.0498 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "3505/3505 [==============================] - 5s 2ms/step - loss: 0.0499 - acc: 0.9963 - val_loss: 0.0464 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0510 - acc: 0.9946 - val_loss: 0.0565 - val_acc: 0.9923\n",
      "**************SET***************::::::: 3\n",
      "Train on 3505 samples, validate on 390 samples\n",
      "Epoch 1/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0488 - acc: 0.9949 - val_loss: 0.0409 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0452 - acc: 0.9957 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0457 - acc: 0.9954 - val_loss: 0.0379 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0431 - acc: 0.9960 - val_loss: 0.0353 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "3505/3505 [==============================] - 5s 2ms/step - loss: 0.0436 - acc: 0.9940 - val_loss: 0.0338 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "3505/3505 [==============================] - 5s 1ms/step - loss: 0.0423 - acc: 0.9949 - val_loss: 0.0330 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      " 128/3505 [>.............................] - ETA: 4s - loss: 0.0292 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold  \n",
    "\n",
    "kf=KFold(n_splits=10)\n",
    "\n",
    "cnt=0\n",
    "for train,test in kf.split(dataset):\n",
    "    cnt=cnt+1\n",
    "    print(\"**************SET***************:::::::\",cnt)\n",
    "    xtrain,xtest = dataset[train],dataset[test]\n",
    "    ytrain,ytest = labels[train],labels[test]\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding\n",
    "    one_hot_labels = keras.utils.to_categorical(ytrain, num_classes=2)\n",
    "    one_hot_labels_test = keras.utils.to_categorical(ytest, num_classes=2)\n",
    "\n",
    "    # Train the model, iterating on the data in batches of 32 samples\n",
    "    model.fit(xtrain, one_hot_labels, epochs=10,validation_data=(xtest, one_hot_labels_test), batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
